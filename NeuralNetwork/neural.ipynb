{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./digit_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(data[\"label\"], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = pd.concat([data,df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = data_encoded.drop([\"label\"], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_encoded[:1000]\n",
    "data_test.shape\n",
    "test_y = data_test.iloc[:, 784:]\n",
    "test_x = data_test.iloc[:,:784]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_encoded[1000:]\n",
    "data_test.shape\n",
    "train_y = data_train.iloc[:, 784:]\n",
    "train_x = data_train.iloc[:,:784]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "test_x = np.array(test_x)\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'The number is 9')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = int(np.random.random() * data_encoded.shape[0])\n",
    "plt.imshow(np.array(data.iloc[index, 1:]).reshape((28,28)), cmap=\"gray\")\n",
    "plt.title(f\"The number is {data.iloc[index,0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims, method =\"basic\"):\n",
    "    \"\"\"Layer dims contains the dimension of each layer in the neural network\"\"\"\n",
    "    parameters = {}\n",
    "\n",
    "    num_layers = len(layer_dims) # as the input layer is taken as 0\n",
    "    if(method == \"basic\"):\n",
    "        for i in range(1, num_layers):\n",
    "            parameters[f\"W{i}\"] = (np.random.random((layer_dims[i-1], layer_dims[i])) - 0.5).T\n",
    "            parameters[f\"b{i}\"] = (np.random.random((1, layer_dims[i])) - 0.5).T\n",
    "\n",
    "    if(method == \"xavier_normal\"):\n",
    "        for i in range(1, num_layers):\n",
    "            fan_in = layer_dims[i-1]\n",
    "            fan_out = layer_dims[i]\n",
    "            sd = np.sqrt(2/(fan_in + fan_out))\n",
    "            parameters[f\"W{i}\"] = (np.random.normal(0, sd, (layer_dims[i-1], layer_dims[i]))).T\n",
    "            parameters[f\"b{i}\"] = np.zeros((1, layer_dims[i])).T\n",
    "\n",
    "    if(method == \"xavier_uniform\"):\n",
    "        for i in range(1, num_layers):\n",
    "            fan_in = layer_dims[i-1]\n",
    "            fan_out = layer_dims[i]\n",
    "            m, n = -np.sqrt(6/(fan_in+fan_out)), np.sqrt(6/(fan_in+fan_out))\n",
    "            parameters[f\"W{i}\"] = (np.random.normal(m, n, (layer_dims[i-1],layer_dims[i]))).T\n",
    "            parameters[f\"b{i}\"] = np.zeros((1, layer_dims[i])).T\n",
    "    \n",
    "\n",
    "    if(method == \"he_uniform\"):\n",
    "        for i in range(1, num_layers):\n",
    "            fan_in = layer_dims[i-1]\n",
    "            m, n = -np.sqrt(6/(fan_in)), np.sqrt(6/(fan_in))\n",
    "            parameters[f\"W{i}\"] = (np.random.normal(m, n, (layer_dims[i-1],layer_dims[i]))).T\n",
    "            parameters[f\"b{i}\"] = np.zeros((1, layer_dims[i])).T\n",
    "    \n",
    "    if(method == \"he_normal\"):\n",
    "        for i in range(1, num_layers):\n",
    "            fan_in = layer_dims[i-1]\n",
    "            sd = np.sqrt(2/(fan_in))\n",
    "            parameters[f\"W{i}\"] = (np.random.normal(0, sd, (layer_dims[i-1], layer_dims[i]))).T\n",
    "            parameters[f\"b{i}\"] = np.zeros((1, layer_dims[i])).T\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True)) # For numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/ (1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sigmoid(z):\n",
    "    sigmoid = 1/ (1+np.exp(-z))\n",
    "    return sigmoid*(1-sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return (np.exp(z) - np.exp(-z))/ (np.exp(z) + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_tanh(z):\n",
    "    tanh = (np.exp(z) - np.exp(-z))/ (np.exp(z) + np.exp(-z))\n",
    "    return (1- tanh**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_relu(z):\n",
    "    return z > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, dropout_rate):\n",
    "    dropout_mask = (np.random.rand(*X.shape) < 1 - dropout_rate)\n",
    "    X *= dropout_mask / (1 - dropout_rate)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_test(X, parameters, dropout_rate):\n",
    "    cache = {}\n",
    "\n",
    "    num_layers = len(parameters)//2\n",
    "\n",
    "    A_final = 0\n",
    "\n",
    "    for i in range(1, len(parameters)//2): \n",
    "        W = parameters[f\"W{i}\"]\n",
    "        b = parameters[f\"b{i}\"]\n",
    "\n",
    "        print(f\"W shape : {W.shape}  X shape = {X.shape}\")\n",
    "        Z = np.dot(W, X) + b\n",
    "        A = relu(Z)\n",
    "        print(f\"A shape : {A.shape}\")\n",
    "        X = A\n",
    "\n",
    "        # Apply dropout to the hidden layer activations\n",
    "        A = dropout(A, dropout_rate)\n",
    "\n",
    "        cache[f\"Z{i}\"] = Z\n",
    "        cache[f\"A{i}\"] = A\n",
    "\n",
    "        A_final = A\n",
    "        print(f\"A_final shape : {A_final.shape}\")\n",
    "\n",
    "\n",
    "    W_last = parameters[f\"W{num_layers}\"]\n",
    "    print(f\"W_last shape : {W_last.shape}\")\n",
    "    b_last = parameters[f\"b{num_layers}\"]\n",
    "\n",
    "\n",
    "    print(f\"A_final shape : {A_final.shape}\")\n",
    "    Z_last = np.dot(W_last, A_final) + b_last\n",
    "    A_last = softmax(Z_last)\n",
    "\n",
    "    cache[f\"Z{num_layers}\"] = Z_last\n",
    "    cache[f\"A{num_layers}\"] = A_last\n",
    "\n",
    "    return cache, A_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters, dropout):\n",
    "    _, _, _, A2 = forward_pass_test(X, parameters, dropout)\n",
    "    return np.argmax(A2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(train_x, train_y, test_x, test_y, trained_parameters, dropout_rate, epoch,loss):\n",
    "        predictions_test = predict(test_x.T, trained_parameters, dropout_rate)\n",
    "        predictions_train = predict(train_x.T, trained_parameters, dropout_rate)\n",
    "        validation_accuracy = np.mean(predictions_train == np.argmax(train_y.T, axis=0))\n",
    "        test_accuracy = np.mean(predictions_test == np.argmax(test_y.T, axis=0))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}, Validation Accuracy : {validation_accuracy * 100:.2f}%,Test Accuracy: {test_accuracy * 100:.2f}%\" )\n",
    "        return test_accuracy, validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass_test(X,Y,parameters,cache):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    temp = {}\n",
    "    out = {}\n",
    "\n",
    "    num_layers = len(parameters)//2\n",
    "\n",
    "\n",
    "    for layer in range(num_layers, 0, -1):\n",
    "        \n",
    "        if layer == num_layers:\n",
    "            temp[f\"dZ{layer}\"] = cache[f\"A{layer}\"] - Y\n",
    "            out[f\"dW{layer}\"] = np.dot(temp[f\"dZ{layer}\"], cache[f\"A{layer - 1}\"].T) /m\n",
    "            out[f\"db{layer}\"] = np.sum(temp[f\"dZ{layer}\"], axis = 1, keepdims = True) / m\n",
    "\n",
    "        elif layer > 1:\n",
    "            temp[f\"dA{layer}\"] = np.dot(parameters[f\"W{layer+1}\"].T, temp[f\"dZ{layer+1}\"])\n",
    "            temp[f\"dZ{layer}\"] = temp[f\"dA{layer}\"] * derivative_relu(cache[f\"Z{layer}\"])\n",
    "            out[f\"dW{layer}\"] = np.dot(temp[f\"dZ{layer}\"], cache[f\"A{layer-1}\"].T) / m\n",
    "            out[f\"db{layer}\"] = np.sum(temp[f\"dZ{layer}\"], axis = 1, keepdims = True) / m\n",
    "\n",
    "        else:\n",
    "            temp[f\"dA{layer}\"] = np.dot(parameters[f\"W{layer+1}\"].T, temp[f\"dZ{layer + 1}\"])\n",
    "            temp[f\"dZ{layer}\"] =  temp[f\"dA{layer}\"] * derivative_relu(cache[f\"Z{layer}\"])\n",
    "            # dW1 = np.dot(dZ1, X.T) / m\n",
    "            # db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "            out[f\"dW{layer}\"] = np.dot(temp[f\"dZ{layer}\"], X.T) / m\n",
    "            out[f\"db{layer}\"] = np.sum(temp[f\"dZ{layer}\"], axis=1, keepdims=True) / m\n",
    "            \n",
    "    # Z1 = cache[\"Z1\"]\n",
    "    # A1 = cache[\"A1\"]\n",
    "    # Z2 = cache[\"Z2\"]\n",
    "    # A2 = cache[\"A2\"]\n",
    "    # Z3 = cache[\"Z3\"]\n",
    "    # A3 = cache[\"A3\"]\n",
    "\n",
    "\n",
    "    # W1 = parameters[\"W1\"]\n",
    "    # b1 = parameters[\"b1\"]\n",
    "    # W2 = parameters[\"W2\"]\n",
    "    # b2 = parameters[\"b2\"]\n",
    "    # W3 = parameters[\"W3\"]\n",
    "    # b3 = parameters[\"b3\"]\n",
    "\n",
    "    # print(f\"A2 = {A2.shape}   y = {Y.shape}\")\n",
    "    # dZ3 = A3 - Y\n",
    "    # dW3 = np.dot(dZ3, A2.T) / m\n",
    "    # db3 = np.sum(dZ3, axis=1, keepdims=True) / m\n",
    "\n",
    "    # dA2 = np.dot(W3.T, dZ3)\n",
    "    # dZ2 = dA2 * derivative_relu(Z2)  # Derivative of ReLU\n",
    "    # dW2 = np.dot(dZ2, A1.T) / m\n",
    "    # db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "\n",
    "    # dA1 = np.dot(W2.T, dZ2)\n",
    "    # dZ1 = dA1 * derivative_relu(Z1)  # Derivative of ReLU\n",
    "    # dW1 = np.dot(dZ1, X.T) / m\n",
    "    # db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "    # gradients = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2, \"dW3\": dW3, \"db3\": db3}\n",
    "\n",
    "    gradients = out\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(Y_pred, Y_true):\n",
    "    m = Y_true.shape[1]\n",
    "    loss = -np.sum(Y_true * np.log(Y_pred + 1e-8)) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_test(parameters, gradients, learning_rate):\n",
    "    cache = {}\n",
    "    for i in range(1, len(parameters)//2 + 1):\n",
    "        w = parameters[f\"W{i}\"]\n",
    "        b = parameters[f\"b{i}\"]\n",
    "\n",
    "        dw = gradients[f\"dW{i}\"]\n",
    "        db = gradients[f\"db{i}\"]\n",
    "\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "\n",
    "        cache[f\"W{i}\"] = w\n",
    "        cache[f\"W{i}\"] = b\n",
    "\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, hidden_size, output_size, learning_rate, epochs, dropout_rate):\n",
    "    input_size = X.shape[0]\n",
    "    parameters = initialize_parameters([input_size, hidden_size, output_size])\n",
    "\n",
    "    test_accuracy_list = []\n",
    "    validation_accuracy_list = []\n",
    "    for epoch in tqdm_notebook(range(epochs), desc=\"Training progress: \"):\n",
    "        cache, A = forward_pass_test(X, parameters, dropout_rate)\n",
    "        loss = cross_entropy_loss(A, Y)\n",
    "        gradients = backward_pass_test(X, Y, parameters, cache)\n",
    "        print(gradients)\n",
    "        parameters = update_parameters_test(parameters, gradients, learning_rate)\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            trained_parameters = parameters\n",
    "            test_accuracy, validation_accuracy= accuracy(train_x, train_y, test_x, test_y,trained_parameters, dropout_rate, epoch, loss)\n",
    "            test_accuracy_list.append(test_accuracy)\n",
    "            validation_accuracy_list.append(validation_accuracy)\n",
    "    return parameters, test_accuracy_list, validation_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(test_x, test_y, trained_parameters, index):\n",
    "    value = predict(test_x.T, trained_parameters, 0)\n",
    "    prediction = value[index]\n",
    "    actual = np.argmax(test_y[index])\n",
    "    plt.title(f\" Actual : {actual}   Prediction : {prediction}\")\n",
    "    plt.imshow(np.array(test_x[index]).reshape((28,28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(values, x_label, y_label, title):\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd0d67a57df4b0cb80240ebee019259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W shape : (64, 784)  X shape = (784, 41000)\n",
      "A shape : (64, 41000)\n",
      "A_final shape : (64, 41000)\n",
      "W_last shape : (10, 64)\n",
      "A_final shape : (64, 41000)\n",
      "{'dW2': array([[-8.11346926e+00,  6.95860881e+01,  4.98113220e+01,\n",
      "         4.30104498e+00,  8.04747506e+00,  9.95355603e-01,\n",
      "         1.03172178e+01,  4.83469960e+01,  1.56091266e+00,\n",
      "         9.67366542e+00,  2.26312364e+01,  3.32494674e+00,\n",
      "         7.56703255e+01,  3.41959012e+01,  7.15750318e+00,\n",
      "         7.00838639e+01,  1.78440344e+01,  4.91773095e+01,\n",
      "         3.90303062e+01,  1.39119175e+02,  4.49653276e+01,\n",
      "         1.68231159e+01,  7.41632807e+01,  5.36805107e+01,\n",
      "         1.35849817e+00,  1.28060864e+01,  6.45694906e+01,\n",
      "         6.11101857e+01,  1.64445130e+02,  7.38923757e+01,\n",
      "         5.15253510e+01,  2.71643469e+01,  1.06902874e+02,\n",
      "         5.55736046e+01,  7.62816321e+00,  9.05719084e+01,\n",
      "        -8.71487064e+00,  4.56950534e+01,  1.49523397e+01,\n",
      "         9.14755662e+00,  5.81857271e+00,  5.02227882e+01,\n",
      "         1.92580722e+01,  1.10751866e+01,  4.03952891e+00,\n",
      "         3.80670984e+01,  2.29833492e+01,  2.30619488e+01,\n",
      "         6.80838466e+01,  1.19578227e+02, -5.69995525e+00,\n",
      "         1.30495321e+01,  5.51410463e+01,  2.06691031e+01,\n",
      "         2.28810449e+01,  3.80708776e+01,  4.51345883e+01,\n",
      "         2.10307912e+01,  2.31701069e+01,  1.38857262e+02,\n",
      "         7.68580696e+01,  3.54229235e+01,  1.07875250e+01,\n",
      "         4.54950060e+01],\n",
      "       [-3.36454900e-01, -8.50709165e+00, -2.95899482e+01,\n",
      "        -2.21393933e+01, -1.25226693e+01, -1.43177363e+00,\n",
      "        -1.73391747e+01, -1.69855133e+01, -8.98051574e-01,\n",
      "         4.22017281e-02, -8.45223584e+00, -3.65203286e+00,\n",
      "        -6.57660822e+00, -6.52263204e+01, -7.64900543e-01,\n",
      "        -7.63981122e+01, -4.20803162e-01, -6.07450760e+01,\n",
      "        -3.44374648e+01, -1.65333111e+01, -2.95417891e+00,\n",
      "        -1.38670827e+01, -2.46903414e+01, -2.56757412e+01,\n",
      "        -4.88327922e-01, -3.20918334e+00, -3.33089690e+00,\n",
      "        -1.10937040e+02, -1.24866311e+02, -6.10724617e+01,\n",
      "        -3.01402760e+01, -1.61821205e+01, -6.05305531e+01,\n",
      "        -4.63439786e+01, -5.65349364e+01, -1.89767605e+01,\n",
      "        -4.02784546e+00, -1.58088706e+01, -3.52244604e-01,\n",
      "        -1.47837302e+01, -2.42459668e-01, -1.82612282e+01,\n",
      "        -4.03375887e-02, -1.12910178e+01, -9.72546190e+00,\n",
      "        -1.80018112e+01, -3.34068835e+01, -3.02152441e+01,\n",
      "        -5.93129360e+01, -8.99112702e+01, -4.38261406e+01,\n",
      "        -2.98746978e+01, -4.14000073e+00, -9.45715313e+00,\n",
      "        -6.56359385e+00, -5.17571122e-02, -7.82113615e+00,\n",
      "        -7.24819339e+00, -1.88214259e+01, -6.68503147e+01,\n",
      "        -1.18240209e+01, -4.03425761e+01, -1.02460592e+00,\n",
      "        -1.39887827e+01],\n",
      "       [ 1.19707498e+01,  3.52722161e+01,  3.39085920e+01,\n",
      "        -1.57626391e+01,  4.08300882e+00, -1.48996854e+00,\n",
      "         8.13819441e+00,  1.68539412e+01,  8.13610901e-01,\n",
      "         6.15812497e+00,  7.08976659e+00,  2.03647257e+00,\n",
      "         4.48307420e+01,  2.55658528e+01, -4.65917336e-01,\n",
      "         4.46943650e+01,  7.48142899e+00,  4.03791569e+01,\n",
      "         1.93819419e+01,  3.99217692e+01,  1.63774510e+01,\n",
      "         1.03622022e+01,  3.17458021e+01,  1.34237645e+01,\n",
      "        -8.30197055e-01,  8.31921115e+00,  6.70538670e+00,\n",
      "         7.45242351e+01,  1.14362640e+02,  6.26494781e+01,\n",
      "         2.18151255e+01,  1.11361755e+01,  2.82879131e+01,\n",
      "        -1.19594515e+00,  1.40483651e+01,  2.51840105e+01,\n",
      "         3.62582248e+00,  2.34645217e+01, -2.07285805e+00,\n",
      "         1.25126155e+01,  1.71836021e+00, -1.73503175e+01,\n",
      "         5.64323577e+00, -1.80970151e+00,  6.77437502e-01,\n",
      "        -6.42962160e+00,  3.60693311e+01,  2.08383558e+01,\n",
      "         3.45464389e+00,  4.23734902e+01,  3.38177305e+01,\n",
      "         1.01842201e+01,  2.30127147e+01,  1.32201974e+01,\n",
      "         1.40949303e+01,  2.31582434e+01,  8.29403103e+00,\n",
      "         7.09660157e+00, -5.71273195e+00,  7.83101586e+01,\n",
      "         2.54516422e+01,  3.08835569e+01,  5.44052236e+00,\n",
      "        -7.47521725e+00],\n",
      "       [-4.35007463e+00, -5.52588780e+01, -1.14245217e+01,\n",
      "        -3.07586198e+01, -6.18298073e-01, -5.43824812e+00,\n",
      "        -2.76013607e+01, -5.39289248e+00, -3.09990312e-01,\n",
      "        -5.24024286e+00, -4.29852706e+00, -1.04070325e-01,\n",
      "        -5.73355408e+01, -1.47086936e+01,  7.50523068e-03,\n",
      "        -3.17915242e+01,  4.10299113e-01, -7.21730279e+01,\n",
      "        -2.98498236e+01, -3.62547665e+01, -8.68938432e+00,\n",
      "        -1.80187390e+01, -8.65420924e+01, -2.86014331e+01,\n",
      "         2.50925801e-01, -5.35636585e-01, -2.02872730e+00,\n",
      "        -6.75398404e+01, -1.47119960e+02, -7.07443900e+01,\n",
      "        -2.77062342e+01, -9.78325197e+00, -9.70828496e+01,\n",
      "        -2.64643677e+01, -6.21286449e+01, -1.30654455e+01,\n",
      "        -1.51244233e+01, -2.12841624e+01, -1.23436438e+00,\n",
      "        -6.48087794e+00, -9.44846535e-01, -6.74233231e+00,\n",
      "         2.65649871e-01, -1.43517246e+01, -4.50351733e+00,\n",
      "        -2.30102590e+01, -4.98675564e+01, -2.55487813e+01,\n",
      "        -5.61389437e+01, -9.47977329e+01, -6.36432706e+01,\n",
      "        -1.10100031e-02, -4.47732422e+01, -4.29174762e+01,\n",
      "        -3.31946335e+00, -1.08430133e+00, -1.18430723e+00,\n",
      "        -8.56000453e+00, -5.34976425e-01, -6.25858105e+01,\n",
      "        -2.47013425e+01, -4.30118454e+01,  6.86621692e-01,\n",
      "        -3.62402830e+01],\n",
      "       [-2.91010645e+00, -4.39321890e+01, -2.10290640e+01,\n",
      "        -2.13095728e+01, -6.87428170e+00,  2.56914336e-01,\n",
      "        -1.04043029e+01, -4.17107838e+01, -2.34255550e+00,\n",
      "        -7.04295441e+00, -2.33232054e+01, -3.61256252e+00,\n",
      "        -5.68656230e+01, -1.70992861e+01, -3.80215733e+00,\n",
      "        -6.38651685e+01, -1.49922999e+01, -2.42827737e+01,\n",
      "        -8.39227820e+00, -8.99080916e+01, -8.63842735e+01,\n",
      "        -1.85171921e+01, -4.60462342e+01, -3.86829006e+01,\n",
      "        -7.34132112e-01, -2.24823803e+01, -4.66294277e+01,\n",
      "        -2.89345521e+01, -1.00154735e+02, -1.54253786e+01,\n",
      "        -6.93262907e+00, -5.29528724e+00, -6.95670244e+01,\n",
      "        -3.04070719e+01, -5.30230858e+01, -6.08281124e+01,\n",
      "        -8.92084983e+00, -3.69516092e+01, -5.28301484e+00,\n",
      "        -1.89185977e+01, -4.04804703e+00, -1.35253432e+01,\n",
      "        -1.43332917e+01, -4.84497995e+00, -3.17555011e+00,\n",
      "        -2.54062879e+01, -2.77537294e+01, -5.33183851e+00,\n",
      "        -3.47270985e+01, -7.95077849e+01, -4.47961648e+01,\n",
      "        -4.67793052e+00, -2.18808230e+01, -1.76599120e+01,\n",
      "        -1.99907385e+01, -6.20019173e+01, -3.22190430e+01,\n",
      "        -1.30787718e+01, -9.49441384e+00, -5.79268207e+01,\n",
      "        -5.51603155e+01, -2.10873344e+01, -1.43925486e+01,\n",
      "        -7.26816233e+00],\n",
      "       [-2.03701511e+00, -2.73676320e+01, -1.57269409e+01,\n",
      "        -2.85408236e+00,  8.31210059e+00, -1.28592962e+00,\n",
      "        -1.03182698e+00, -5.81570313e+00, -7.80796708e-02,\n",
      "        -6.28956046e+00, -2.59250271e+01, -2.20134769e-01,\n",
      "        -8.02054271e+00, -4.73058860e+00,  7.93266464e-01,\n",
      "        -8.95821224e+00,  1.37701905e+01, -4.03520297e+01,\n",
      "        -1.09616431e+01, -2.28506119e+01,  7.22164940e+00,\n",
      "        -2.17970247e+01, -2.95701042e+01,  1.03808182e+01,\n",
      "         5.78172255e-02,  2.64764047e+00, -1.02004447e+01,\n",
      "        -2.02896282e+01, -5.81256042e+01, -3.36450147e+01,\n",
      "        -3.01274868e+01, -7.14393827e+00, -3.23537066e+01,\n",
      "        -1.65221033e+01,  5.12482024e+00, -1.97857750e+01,\n",
      "        -3.37786376e+00,  9.48891590e+00, -1.26379235e+00,\n",
      "        -6.39958140e+00,  1.30350344e+00, -1.90749055e+00,\n",
      "        -7.20317745e-01,  5.12088286e+00,  3.24275058e+00,\n",
      "        -1.53785084e+01, -1.27622853e+01, -6.96735556e+00,\n",
      "         3.68692474e+00, -1.58307724e+01,  1.53458944e+01,\n",
      "        -5.29179808e+00, -1.43573278e+01, -5.01865591e+00,\n",
      "         8.11509204e+00,  1.16565641e-01, -1.33003931e+01,\n",
      "        -7.85375599e+00,  5.67665807e+00, -6.47036041e+01,\n",
      "         5.28257866e+00, -5.07464128e+01,  3.52436067e+00,\n",
      "        -6.34702622e+00],\n",
      "       [-9.81106489e+00, -6.86186675e+01, -1.25890740e+01,\n",
      "        -1.23928814e+01, -1.49756336e+01, -2.87631606e+00,\n",
      "        -4.94547490e+00, -2.97741396e+01, -1.04753803e+00,\n",
      "        -3.41692083e+00, -1.12709014e+01, -6.94251403e+00,\n",
      "        -3.92946580e+01, -1.82898335e+01, -9.95409466e-01,\n",
      "        -6.26726706e+01, -6.17418170e+00, -5.66402278e+01,\n",
      "        -1.58849302e+01, -8.23068289e+01, -2.42615077e+01,\n",
      "        -1.96623108e+01, -6.37207793e+01, -3.60708013e+01,\n",
      "        -9.39178639e-01, -4.86502488e+00, -3.46834104e+01,\n",
      "        -4.15346981e+01, -1.17232268e+02, -3.40641699e+01,\n",
      "        -3.65439624e+01, -2.26023343e+01, -5.01632688e+01,\n",
      "        -1.76421278e+01, -2.21813580e+01, -4.89920383e+01,\n",
      "        -2.33303885e+00, -8.98388910e+00, -1.80081718e+01,\n",
      "        -1.51875521e+01, -1.67483337e+00, -6.28479073e+01,\n",
      "        -8.32084050e-01, -1.25080585e+01, -1.01495405e+01,\n",
      "        -4.18367747e+01, -2.13318559e+01, -7.10781076e-01,\n",
      "        -6.09501513e+01, -7.63674423e+01, -4.68817165e+01,\n",
      "        -3.27887635e+00, -5.05774557e+01, -3.17907687e+00,\n",
      "        -3.80890808e+01, -2.95756592e+01, -4.74269919e+01,\n",
      "        -2.58924348e+00, -1.07269921e+01, -9.13387938e+01,\n",
      "        -1.35550586e+01, -3.42084930e+01, -2.01535065e+00,\n",
      "        -2.51887656e+01],\n",
      "       [-1.89621711e+00, -9.05488423e+01, -6.78092630e+00,\n",
      "        -4.76249058e+01, -2.39648907e+01, -1.19164154e+00,\n",
      "        -2.44095273e+01, -3.33946334e+01, -2.21273298e-01,\n",
      "        -1.45476964e+00, -1.06567321e+01, -1.65246908e+00,\n",
      "        -7.50322711e+01, -3.13332506e+01, -3.62580354e+00,\n",
      "        -4.10731226e+01, -4.40245011e+01, -4.13728117e+01,\n",
      "        -8.69270816e+00, -6.00652974e+01, -5.56211438e+01,\n",
      "        -9.84748959e+00, -7.06079054e+01, -4.70687008e+01,\n",
      "        -3.94848691e-01, -7.66418429e+00, -1.04065264e+01,\n",
      "        -6.78563981e+01, -1.29927028e+02, -1.06281981e+01,\n",
      "        -3.68674813e+01, -8.17002766e+00, -5.95585783e+01,\n",
      "        -1.40663570e+01, -8.42030308e+01, -2.74313040e+01,\n",
      "        -1.06211912e+01, -7.93521913e+01, -5.25945137e+00,\n",
      "        -1.69606939e+01, -6.03708502e+00, -5.25826581e+00,\n",
      "        -1.11191553e+01, -1.05339969e+01, -1.01132350e+01,\n",
      "        -5.18705235e+00, -1.42620974e+01, -1.04753367e+01,\n",
      "        -6.11218792e+01, -6.96394636e+01, -7.94031375e+01,\n",
      "        -6.76521236e+00, -3.07548441e+01, -9.98196804e+00,\n",
      "        -2.00125813e+01, -1.40712981e+01, -3.42381798e+01,\n",
      "        -2.47797927e+01, -9.27926405e+00, -8.65110928e+01,\n",
      "        -4.48718428e+01, -1.10357469e+01, -1.99600525e+01,\n",
      "        -6.18170587e+00],\n",
      "       [-7.49222624e+00, -4.93742285e+01, -2.46040980e+01,\n",
      "        -2.28883918e+01,  3.63763532e+00, -5.36752285e-01,\n",
      "        -1.33683129e+01, -1.98137853e+01, -2.87812129e-01,\n",
      "        -5.25934381e-01, -2.40376577e+01,  2.82282769e-02,\n",
      "        -3.43295114e+01, -3.43388882e+01,  2.54766785e-01,\n",
      "        -5.45625546e+01, -6.58355510e-01, -5.81179266e+01,\n",
      "        -2.00242767e+01, -4.91616147e+01,  5.35713985e+00,\n",
      "        -1.44558372e+01, -8.29479434e+01, -2.50008848e+01,\n",
      "         3.27174363e-02,  2.01389069e-01, -4.47309483e-01,\n",
      "        -5.13871554e+01, -1.13931193e+02, -3.99518686e+01,\n",
      "        -1.77325207e+01, -3.37687434e+01, -3.22644814e+01,\n",
      "        -3.40896587e+01, -6.36255396e+01, -6.03918343e+00,\n",
      "        -4.38295975e+00, -2.50168646e+01, -6.17202465e-01,\n",
      "        -9.10367370e+00,  1.38199399e-01, -1.22408317e+01,\n",
      "        -2.99171808e-01, -6.82525687e+00,  3.11320651e+00,\n",
      "        -1.50575587e+01, -4.97990179e+01, -6.23789875e+00,\n",
      "        -1.52749169e+01, -7.23888066e+01, -5.08187881e+01,\n",
      "        -7.22040313e+00, -3.72244457e+00, -8.67147510e+00,\n",
      "         1.30910459e+00,  9.76166098e+00,  1.39827922e+00,\n",
      "        -7.09800150e+00, -2.88237410e+00, -7.18430084e+01,\n",
      "        -1.23379598e+01, -4.80950071e+01,  6.83576780e-01,\n",
      "        -9.27312326e+00],\n",
      "       [ 2.49758788e+01,  2.38749225e+02,  3.80246590e+01,\n",
      "         1.71429441e+02,  3.48755536e+01,  1.29983599e+01,\n",
      "         8.06445681e+01,  8.76865138e+01,  2.81077695e+00,\n",
      "         8.09639046e+00,  7.82432836e+01,  1.07941360e+01,\n",
      "         1.56953688e+02,  1.25965107e+02,  1.44114656e+00,\n",
      "         2.24543136e+02,  2.67641884e+01,  2.64127407e+02,\n",
      "         6.98308766e+01,  1.78039578e+02,  1.03988920e+02,\n",
      "         8.89803581e+01,  2.98216318e+02,  1.23615368e+02,\n",
      "         1.68672578e+00,  1.47820823e+01,  3.64518655e+01,\n",
      "         2.52844891e+02,  5.12549330e+02,  1.28989628e+02,\n",
      "         1.12710114e+02,  6.46451809e+01,  2.66329675e+02,\n",
      "         1.31158006e+02,  3.14895247e+02,  7.93627002e+01,\n",
      "         5.38772203e+01,  1.08749096e+02,  1.91387602e+01,\n",
      "         6.61745350e+01,  3.96863587e+00,  8.79109284e+01,\n",
      "         2.17740040e+00,  4.59686666e+01,  2.65943813e+01,\n",
      "         1.12240775e+02,  1.50130746e+02,  4.15869314e+01,\n",
      "         2.12300510e+02,  3.36491556e+02,  2.85905549e+02,\n",
      "         3.38861761e+01,  9.20523772e+01,  6.29964167e+01,\n",
      "         4.15752860e+01,  3.56775853e+01,  8.13631527e+01,\n",
      "         4.30803707e+01,  2.86054135e+01,  2.84592024e+02,\n",
      "         5.48582497e+01,  1.82220935e+02,  1.62699511e+01,\n",
      "         6.64680602e+01]]), 'db2': array([[ 0.1595656 ],\n",
      "       [-0.11139915],\n",
      "       [ 0.07348147],\n",
      "       [-0.08347985],\n",
      "       [-0.09221687],\n",
      "       [-0.03524333],\n",
      "       [-0.097739  ],\n",
      "       [-0.1047561 ],\n",
      "       [-0.06887306],\n",
      "       [ 0.36066028]]), 'dW1': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), 'db1': array([[ 0.0075636 ],\n",
      "       [ 0.11593427],\n",
      "       [-0.02938802],\n",
      "       [ 0.07250834],\n",
      "       [-0.00314325],\n",
      "       [ 0.01154309],\n",
      "       [ 0.05858105],\n",
      "       [ 0.09774177],\n",
      "       [-0.00289043],\n",
      "       [-0.00253068],\n",
      "       [ 0.0103733 ],\n",
      "       [-0.03329462],\n",
      "       [-0.11822117],\n",
      "       [ 0.02738261],\n",
      "       [ 0.01885108],\n",
      "       [ 0.03887802],\n",
      "       [ 0.05341799],\n",
      "       [ 0.02452003],\n",
      "       [-0.05781457],\n",
      "       [ 0.01776203],\n",
      "       [-0.02423106],\n",
      "       [-0.06346865],\n",
      "       [ 0.10707181],\n",
      "       [ 0.05751693],\n",
      "       [-0.00548091],\n",
      "       [-0.00331938],\n",
      "       [ 0.01335573],\n",
      "       [ 0.14380973],\n",
      "       [ 0.15222848],\n",
      "       [ 0.06662223],\n",
      "       [ 0.16144963],\n",
      "       [ 0.05120434],\n",
      "       [ 0.08042781],\n",
      "       [ 0.16468413],\n",
      "       [ 0.19476044],\n",
      "       [ 0.04085961],\n",
      "       [-0.05611932],\n",
      "       [ 0.05431248],\n",
      "       [-0.00410364],\n",
      "       [-0.09409648],\n",
      "       [ 0.01995879],\n",
      "       [ 0.04212585],\n",
      "       [-0.01996873],\n",
      "       [-0.05000331],\n",
      "       [-0.03308658],\n",
      "       [ 0.04431852],\n",
      "       [-0.11773659],\n",
      "       [ 0.0671555 ],\n",
      "       [ 0.00072263],\n",
      "       [ 0.15488145],\n",
      "       [ 0.15875597],\n",
      "       [ 0.01200316],\n",
      "       [ 0.03106652],\n",
      "       [-0.0578668 ],\n",
      "       [-0.00189823],\n",
      "       [ 0.02713903],\n",
      "       [ 0.07474526],\n",
      "       [-0.01283408],\n",
      "       [-0.02554858],\n",
      "       [ 0.00510172],\n",
      "       [ 0.00167224],\n",
      "       [ 0.13896039],\n",
      "       [ 0.02041254],\n",
      "       [ 0.12902826]])}\n",
      "W_last shape : (64, 1)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'b1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Python\\DataMining\\NeuralNetwork\\neural.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m epoch \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m100\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dropout_rate \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# Example dropout rate of 20% (0.2)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m trained_parameters, validation_accuracy, test_accuracy \u001b[39m=\u001b[39m train(train_x\u001b[39m.\u001b[39;49mT, train_y\u001b[39m.\u001b[39;49mT, hidden_size, output_size, learning_rate, epoch, dropout_rate)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m predictions \u001b[39m=\u001b[39m predict(test_x\u001b[39m.\u001b[39mT, trained_parameters, dropout_rate)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Calculate accuracy\u001b[39;00m\n",
      "\u001b[1;32md:\\Python\\DataMining\\NeuralNetwork\\neural.ipynb Cell 28\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, Y, hidden_size, output_size, learning_rate, epochs, dropout_rate)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     trained_parameters \u001b[39m=\u001b[39m parameters\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     test_accuracy, validation_accuracy\u001b[39m=\u001b[39m accuracy(train_x, train_y, test_x, test_y,trained_parameters, dropout_rate, epoch, loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     test_accuracy_list\u001b[39m.\u001b[39mappend(test_accuracy)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     validation_accuracy_list\u001b[39m.\u001b[39mappend(validation_accuracy)\n",
      "\u001b[1;32md:\\Python\\DataMining\\NeuralNetwork\\neural.ipynb Cell 28\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(train_x, train_y, test_x, test_y, trained_parameters, dropout_rate, epoch, loss)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maccuracy\u001b[39m(train_x, train_y, test_x, test_y, trained_parameters, dropout_rate, epoch,loss):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         predictions_test \u001b[39m=\u001b[39m predict(test_x\u001b[39m.\u001b[39;49mT, trained_parameters, dropout_rate)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         predictions_train \u001b[39m=\u001b[39m predict(train_x\u001b[39m.\u001b[39mT, trained_parameters, dropout_rate)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         validation_accuracy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(predictions_train \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39margmax(train_y\u001b[39m.\u001b[39mT, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n",
      "\u001b[1;32md:\\Python\\DataMining\\NeuralNetwork\\neural.ipynb Cell 28\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(X, parameters, dropout)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(X, parameters, dropout):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     _, _, _, A2 \u001b[39m=\u001b[39m forward_pass_test(X, parameters, dropout)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39margmax(A2, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;32md:\\Python\\DataMining\\NeuralNetwork\\neural.ipynb Cell 28\u001b[0m in \u001b[0;36mforward_pass_test\u001b[1;34m(X, parameters, dropout_rate)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m W_last \u001b[39m=\u001b[39m parameters[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m{\u001b[39;00mnum_layers\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mW_last shape : \u001b[39m\u001b[39m{\u001b[39;00mW_last\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m b_last \u001b[39m=\u001b[39m parameters[\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mb\u001b[39;49m\u001b[39m{\u001b[39;49;00mnum_layers\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mA_final shape : \u001b[39m\u001b[39m{\u001b[39;00mA_final\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Python/DataMining/NeuralNetwork/neural.ipynb#Y133sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m Z_last \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(W_last, A_final) \u001b[39m+\u001b[39m b_last\n",
      "\u001b[1;31mKeyError\u001b[0m: 'b1'"
     ]
    }
   ],
   "source": [
    "input_size = train_x.shape[1]\n",
    "output_size = train_y.shape[1]\n",
    "\n",
    "hidden_size = 64\n",
    "learning_rate = 0.001\n",
    "iterations = 100\n",
    "epoch = int(100)\n",
    "dropout_rate = 0  # Example dropout rate of 20% (0.2)\n",
    "\n",
    "trained_parameters, validation_accuracy, test_accuracy = train(train_x.T, train_y.T, hidden_size, output_size, learning_rate, epoch, dropout_rate)\n",
    "predictions = predict(test_x.T, trained_parameters, dropout_rate)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == np.argmax(test_y.T, axis=0))\n",
    "print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
